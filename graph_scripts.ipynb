{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab704e6-167e-4f74-94cd-b62c0cfaa9cf",
   "metadata": {},
   "source": [
    "# Gray Scale Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea2827-8866-4d80-a5b4-8f9ae6b27d9c",
   "metadata": {},
   "source": [
    "### Important Variables:\n",
    "\n",
    "In this task we will be implementing the SIR model with the following system parameters:\n",
    "\n",
    "*   $a_{i}(b)$: total amount of assets hold by user i by the end of block b.\n",
    "*   $d_{i}(b)$: total amount of black assets hold by node i by the end of block b.\n",
    "*   $x_{i}(b)$: = $\\frac{d_{i}(b)}{a_{i}(b)} $ black assets fraction by the end of block b.\n",
    "*   $c_{i}(b)$: amount of clean asset owned by node i by the end of block b\n",
    "*   $m_{i}(b)$: the amount mined by user i by the end of block b\n",
    "*   $W(b)$: = {$w_{ij}$}$_{i,j=1,...,N}$ is the adjacency matrix of the transaction network at block b. wij is equal to the amount of bitcoin moved from user i to user j at block b.\n",
    "*   $N = Nusers = |Nusers|$ total number of users in the network.\n",
    "\n",
    "Below is a summary of the dynamics of the SIR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "single-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocksci\n",
    "\n",
    "import sys, os, os.path, socket\n",
    "import numpy as np\n",
    "import zarr\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from decimal import Decimal\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from util import SYMBOLS, DIR_BCHAIN, DIR_PARSED, SimpleChrono, darknet\n",
    "\n",
    "\n",
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd3175",
   "metadata": {},
   "source": [
    "# Address Mapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77686ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class creates a map from addresses to the set {0 -> number of cluster}, in other words it helps to use arrays to store the information. \n",
    "class AddressMapper(): # This creates the map that clusters the addresses to actual users. Treat it a a blackbox for now.\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "\n",
    "        self.__address_types = [blocksci.address_type.nonstandard, blocksci.address_type.pubkey,\n",
    "                                blocksci.address_type.pubkeyhash, blocksci.address_type.multisig_pubkey,\n",
    "                                blocksci.address_type.scripthash, blocksci.address_type.multisig,\n",
    "                                blocksci.address_type.nulldata, blocksci.address_type.witness_pubkeyhash,\n",
    "                                blocksci.address_type.witness_scripthash, blocksci.address_type.witness_unknown]\n",
    "\n",
    "        self.__counter_addresses = { _:self.chain.address_count(_) for _ in self.__address_types }\n",
    "\n",
    "        self.__offsets = {}\n",
    "        offset = 0\n",
    "        for _ in self.__address_types:\n",
    "            self.__offsets[_] = offset\n",
    "            offset += self.__counter_addresses[_]\n",
    "\n",
    "\n",
    "        self.total_addresses = offset\n",
    "        print(f\"[INFO] #addresses: {self.total_addresses}\")\n",
    "#        print(self.__counter_addresses)\n",
    "\n",
    "\n",
    "    def map_clusters(self,cm):\n",
    "#        address_vector = {_: np.zeros(self.__counter_addresses[_], dtype=np.int64) for _ in self.__address_types }\n",
    "        cluster_vector = {_: np.zeros(self.__counter_addresses[_], dtype=np.int64) for _ in self.__address_types }\n",
    "\n",
    "        self.cluster = np.zeros(self.total_addresses, dtype=np.int64)\n",
    "        offset = 0\n",
    "        for _at in cluster_vector.keys():\n",
    "            clusters = cluster_vector[_at]\n",
    "            print(f\"{_at}     -  {len(clusters)}\")\n",
    "#            addrs = address_vector[_at]\n",
    "            for _i, _add in enumerate(chain.addresses(_at)):\n",
    "#                addrs[_i] = _add.address_num\n",
    "                clusters[_i] = cm.cluster_with_address(_add).index\n",
    "                #max_addr_num = max(max_addr_num, addrs[_i])\n",
    "#        pickle.dump(cluster_vector, open(\"cluster_dict.pickle\",\"wb\"))\n",
    "\n",
    "        offset = 0\n",
    "        for _ in cluster_vector.keys():\n",
    "            v = cluster_vector[_]\n",
    "            self.cluster[offset:offset + len(v)] = v\n",
    "            offset += len(v)\n",
    "\n",
    "\n",
    "\n",
    "    def dump_clusters(self, output_folder):\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.mkdir(output_folder)\n",
    "        zarr.save(f\"{output_folder}/address_cluster_map.zarr\", self.cluster)\n",
    "\n",
    "\n",
    "#    def dump_offsets(self, output_folder):\n",
    "#        if not os.path.exists(output_folder):\n",
    "#            os.mkdir(output_folder)\n",
    "#        pickle.dump(self.__offsets, open(f\"{output_folder}/offsets.pickle\", \"wb\") )\n",
    "\n",
    "#    def load_offsets(self, output_folder):\n",
    "#        if not os.path.exists(output_folder):\n",
    "#            os.mkdir(output_folder)\n",
    "#        self.__offsets = pickle.load( open(f\"{output_folder}/offsets.pickle\", \"rb\") )\n",
    "\n",
    "    def load_clusters(self, input_folder):\n",
    "        print(f\"{input_folder}address_cluster_map.zarr\")\n",
    "        self.cluster = zarr.load(f\"{input_folder}address_cluster_map.zarr\")\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self,addr):\n",
    "        return self.__offsets[addr.raw_type]+ addr.address_num-1\n",
    "\n",
    "def catch(address, am):\n",
    "    try:\n",
    "        return am[address] \n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51555405",
   "metadata": {},
   "source": [
    "## Total number of addresses and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be28a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] #addresses: 4022303272\n",
      "/export/consensus-2/blockchain_parsed/bitcoin/heur_1_data/address_cluster_map.zarr\n",
      "[INFO] #clusters: 591692716\n"
     ]
    }
   ],
   "source": [
    "chain = blocksci.Blockchain(f\"{DIR_PARSED}/bitcoin_old.cfg\") # load the blockchain\n",
    "am = AddressMapper(chain)\n",
    "am.load_clusters(f\"{DIR_PARSED}/bitcoin/heur_1_data/\") #this data should already be on the server.\n",
    "\n",
    "no_clusters = max( am.cluster ) + 1 # print the number of clusters\n",
    "print(f\"[INFO] #clusters: {no_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a54fc-3c4c-433d-ad7f-057121534b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Counting number of True and False values in cluster_is_black_ground_truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ddf0326-92d7-43d9-b35a-863220f12366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591692716\n",
      "591692716\n",
      "591692581\n",
      "135\n",
      "[False False False ... False False False]\n",
      "[ 79800777  82285475  86826333  87290351  88316586  89576173  90613521\n",
      "  92044383  93820427  93821719  93822603  94285328  94377857  94494584\n",
      "  94507944  94616297  94620783  94624325  94668900  94797389  94887197\n",
      "  94890855  94892403  94894707  94897322  94898493  94898632  94900520\n",
      "  94903701  94904451  94905527  94907956  94910188  94912551  94913697\n",
      "  94913957  94914365  94915941  94918924  94918931  94921217  94922115\n",
      "  94922151  94922197  94922679  94926589  94927243  94928208  94934381\n",
      "  94934648  94937864  94939326  94941929  94943306  94943818  94944952\n",
      "  94944968  94946270  94946799  94947040  94947412  94947783  94948795\n",
      "  94949293  94949444  94950027  94950285  94950484  94951587  94952593\n",
      "  94954100  94959563  94960057  94961888  94962603  94963307  94966001\n",
      "  94966614  94967639  94968329  94969379  94969678  94969686  94971061\n",
      "  94971748  94972606  94972700  94974446  94974455  94974547  94974967\n",
      "  94975333  94977365  94977422  94977574  94978733  94978757  94978840\n",
      "  94979476  94979978  94981133  94981757  94981858  94982138  94982403\n",
      "  94982761  94982968  94983071  94984257  94984615  94985115  94985299\n",
      "  94985594  94985822  94986385  94987067  94987784  94998167  95268784\n",
      "  95275069  95276912  95277585  96015866  97655034  97655136  99055654\n",
      "  99061401  99061408  99064866  99066985  99422262 101080866 107302595\n",
      " 112150330 115726064]\n"
     ]
    }
   ],
   "source": [
    "array_zarr = zarr.load('uniform_black/heur_1_data/cluster_is_black_ground_truth.zarr') #load zarr file into array\n",
    "df = pd.DataFrame (array_zarr) ## convert your array into a dataframe\n",
    "print(len(df.index))\n",
    "#filepath = 'zarr_to_excel.csv'\n",
    "#df.to_csv(filepath, index=False)\n",
    "countTrue = np.where(array_zarr==True)\n",
    "countFalse = np.where(array_zarr==False)\n",
    "element_count_False = countFalse[0].size\n",
    "element_count_True = countTrue[0].size\n",
    "print(len(array_zarr))\n",
    "print(element_count_False)\n",
    "print(element_count_True)\n",
    "print(array_zarr)\n",
    "for i in countTrue:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ddf0326-92d7-43d9-b35a-863220f12366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28027931\n",
      "28027931\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{DIR_PARSED}/bitcoin_darknet/ground_truth_id.csv\")\n",
    "\n",
    "print(len(df.index))\n",
    "print(df.shape[0])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0240335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822562\n",
      "1822562\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"uniform_black/heur_1_data/ground_truth_clust_id.csv\")\n",
    "\n",
    "print(len(df.index))\n",
    "print(df.shape[0])\n",
    "# print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e74e244f4ccea5b4a2f45c3e3fdabd73b82aec326271051b22d3feb4931efa32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
